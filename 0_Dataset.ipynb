{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"0_Dataset.ipynb","provenance":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HvOAxwdeOG9H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595147965043,"user_tz":-600,"elapsed":1175,"user":{"displayName":"Seyed Assadzadeh Birjandi","photoUrl":"","userId":"04866360597418250923"}},"outputId":"746171b4-41ce-4c91-9556-70f29e05381e"},"source":["# Mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# change working directory\n","%cd \"/content/drive/My Drive/Colab Notebooks/Computer Vision Nanodegree Udacity/Project_2_Image_Captioning\"\n","\n","# install requirements\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/Colab Notebooks/Computer Vision Nanodegree Udacity/Project_2_Image_Captioning\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FrTg8z20OF8H","colab_type":"text"},"source":["# Computer Vision Nanodegree\n","\n","## Project: Image Captioning\n","\n","---\n","\n","The Microsoft **C**ommon **O**bjects in **CO**ntext (MS COCO) dataset is a large-scale dataset for scene understanding.  The dataset is commonly used to train and benchmark object detection, segmentation, and captioning algorithms.  \n","\n","![Sample Dog Output](images/coco-examples.jpg)\n","\n","You can read more about the dataset on the [website](http://cocodataset.org/#home) or in the [research paper](https://arxiv.org/pdf/1405.0312.pdf).\n","\n","In this notebook, you will explore this dataset, in preparation for the project.\n","\n","## Step 1: Initialize the COCO API\n","\n","We begin by initializing the [COCO API](https://github.com/cocodataset/cocoapi) that you will use to obtain the data."]},{"cell_type":"code","metadata":{"id":"9ndTqVjJOF8I","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","sys.path.append('/opt/cocoapi/PythonAPI')\n","from pycocotools.coco import COCO\n","\n","# initialize COCO API for instance annotations\n","dataDir = '/opt/cocoapi'\n","dataType = 'val2014'\n","instances_annFile = os.path.join(dataDir, 'annotations/instances_{}.json'.format(dataType))\n","coco = COCO(instances_annFile)\n","\n","# initialize COCO API for caption annotations\n","captions_annFile = os.path.join(dataDir, 'annotations/captions_{}.json'.format(dataType))\n","coco_caps = COCO(captions_annFile)\n","\n","# get image ids \n","ids = list(coco.anns.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mPRetdlJOF8M","colab_type":"text"},"source":["## Step 2: Plot a Sample Image\n","\n","Next, we plot a random image from the dataset, along with its five corresponding captions.  Each time you run the code cell below, a different image is selected.  \n","\n","In the project, you will use this dataset to train your own model to generate captions from images!"]},{"cell_type":"code","metadata":{"id":"WTdoatK3OF8M","colab_type":"code","colab":{}},"source":["import numpy as np\n","import skimage.io as io\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# pick a random image and obtain the corresponding URL\n","ann_id = np.random.choice(ids)\n","img_id = coco.anns[ann_id]['image_id']\n","img = coco.loadImgs(img_id)[0]\n","url = img['coco_url']\n","\n","# print URL and visualize corresponding image\n","print(url)\n","I = io.imread(url)\n","plt.axis('off')\n","plt.imshow(I)\n","plt.show()\n","\n","# load and display captions\n","annIds = coco_caps.getAnnIds(imgIds=img['id']);\n","anns = coco_caps.loadAnns(annIds)\n","coco_caps.showAnns(anns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wk3g5QGEOF8Q","colab_type":"text"},"source":["## Step 3: What's to Come!\n","\n","In this project, you will use the dataset of image-caption pairs to train a CNN-RNN model to automatically generate images from captions.  You'll learn more about how to design the architecture in the next notebook in the sequence (**1_Preliminaries.ipynb**).\n","\n","![Image Captioning CNN-RNN model](images/encoder-decoder.png)"]}]}